# üñºÔ∏è Image Captioning System

**Advanced Image Captioning System that generates natural language descriptions for any image using state-of-the-art Transformer architecture.**

The model architecture combines EfficientNetB0 CNN for visual feature extraction with a custom Transformer encoder-decoder for sequential text generation. Built entirely with TensorFlow/Keras, this system represents a modern approach to the image-to-text generation task.

---

## üìã Table of Contents

1. üìä [About Dataset](#about-dataset)  
2. üèóÔ∏è [Model Architecture](#model-architecture)  
3. ‚ú® [Key Features](#key-features)  
4. üîÑ [Training Pipeline Overview](#training-pipeline-overview)  
5. üìã [Requirements](#requirements)  
6. üìÅ [Project Structure](#project-structure)  
7. üéØ [Usage](#usage)  
8. üé® [Results](#results)  
9. üìö [References](#references)  
10. üìÑ [License](#license)
11. ü§ù [Contributing](#contributing)

---

## About Dataset

The model is trained on **Flickr8K Dataset** containing:
- 8,000 high-quality images
- 40,000 human-written captions (5 per image)
- Automatic download and preprocessing
- 80/20 train/validation split

*Can be easily adapted for Flickr30K, MS COCO, or custom datasets*

Download instructions and additional details are available here: [Flickr8k Datasets](https://github.com/Avaneesh40585/Image-Captioning/releases/tag/dataset) 

---

## Model Architecture
```
Input Image (299√ó299√ó3)
        ‚Üì
EfficientNetB0 CNN (Feature Extractor)
        ‚Üì
Spatial Features ‚Üí Dense(768D) ‚Üí Reshape
        ‚Üì
Transformer Encoder (Multi-Head Attention)
        ‚Üì
Transformer Decoder
   ‚Üë                       ‚Üì
Text Embeddings +      Masked + 
Positional Encoding    Cross Attention
        ‚Üì
Vocabulary Projection (12K tokens)
        ‚Üì
Generated Caption
```

**Architecture Specifications:**
- **CNN Backbone**: EfficientNetB0 (ImageNet pre-trained, fine-tuned last 30 layers)
- **Transformer Encoder**: 3 layers, 12 attention heads, 768D embeddings
- **Transformer Decoder**: 3 layers with causal masking and cross-attention
- **Text Processing**: 12K vocabulary with learned positional embeddings
- **Feed-Forward Dimension**: 2048D with GELU activation
- **Dropout Rate**: 15% for regularization

**A comprehensive explanation of the code is provided in the notebook itself.**

---

## Key Features

### Advanced Architecture
- **Custom Transformer Implementation**: Hand-built encoder-decoder blocks with proper masking
- **Multi-Caption Training**: Processes 5 captions per image with individual gradient updates
- **Fine-tuned CNN**: EfficientNetB0 with selective layer unfreezing for optimal feature extraction
- **Positional Embeddings**: Learned position encodings for sequence understanding

### Training Optimizations
- **Advanced Learning Rate Scheduling**: Warmup + cosine decay with AdamW optimizer
- **Comprehensive Data Augmentation**: 5-layer pipeline (flip, rotation, contrast, brightness, zoom)
- **Gradient Clipping**: Norm-based clipping for training stability
- **Smart Callbacks**: Early stopping and model checkpointing with best weight restoration

### Evaluation & Testing
- **Multiple Metrics**: BLEU-4 and CIDEr scoring for comprehensive evaluation
- **Flexible Generation**: Both greedy and temperature-based sampling
- **Interactive Testing**: Built-in functions for custom image evaluation
- **Robust Error Handling**: Graceful failure recovery and model saving

---

## Training Pipeline Overview

### Phase 1 ‚Äì Data Setup & Preprocessing

- Downloads **Flickr8K dataset** automatically via `wget` commands (~1GB total)
- Extracts **8,000 images** and **caption files** from zip archives
- Loads and parses `Flickr8k.token.txt` containing **40,000 captions**
- Filters malformed captions and **builds vocabulary** (~12K tokens)
- Creates **train/validation split** (6,400 / 1,600 images)
- Implements **robust error handling** for missing or corrupted files

### Phase 2 ‚Äì Model Architecture Construction

- Builds **EfficientNetB0 CNN** feature extractor with pretrained **ImageNet weights**
- Constructs **custom Transformer encoder blocks** with multi-head self-attention
- Creates **Transformer decoder** with **causal masking** and **cross-attention**
- Sets up **learned positional embeddings** and **text preprocessing pipeline**
- Configures **5-layer image augmentation** strategy
- Initializes complete `ImageCaptioningModel` with all components

### Phase 3 ‚Äì Advanced Training Process

- Implements **learning rate scheduling** (warmup + cosine decay)
- Configures **AdamW optimizer** with **gradient clipping** and **weight decay**
- Processes **5 captions per image** with individual gradient updates
- Applies **comprehensive callbacks** (early stopping, model checkpointing)
- Monitors **training/validation metrics** in real-time
- Automatically saves **best performing model** based on validation loss

### Phase 4 ‚Äì Evaluation & Testing

- Evaluates model performance using **BLEU-4** and **CIDEr** metrics
- Tests on **random validation samples** with **automatic caption generation**
- Provides **interactive testing functions** for custom image inputs
- Implements both **greedy decoding** and **temperature-based sampling**
- Saves **final trained model** and displays comprehensive results
- Generates **performance reports** with detailed metric analysis

---

## Requirements
```
tensorflow>=2.13.0
keras>=2.13.0
numpy>=1.21.0
matplotlib>=3.5.0
nltk>=3.8.0
```

These requirements can be easily installed by: `pip install -r requirements.txt`

---

## Project Structure
```
image-captioning-system/
‚îú‚îÄ‚îÄ Image Caption Generator.ipynb    # Complete implementation notebook
‚îú‚îÄ‚îÄ README.md                        # This file
‚îú‚îÄ‚îÄ requirements.txt                 # Dependencies
‚îú‚îÄ‚îÄ models/                          # Saved models (generated after training)
‚îÇ   ‚îú‚îÄ‚îÄ best_caption_model.keras
‚îÇ   ‚îî‚îÄ‚îÄ final_caption_model.keras
‚îú‚îÄ‚îÄ Flicker8k_Dataset/              # Auto-downloaded image dataset
‚îÇ   ‚îî‚îÄ‚îÄ *.jpg                       # 8,000 images
‚îî‚îÄ‚îÄ Flickr8k_text/                  # Auto-downloaded text dataset
    ‚îú‚îÄ‚îÄ Flickr8k.token.txt         # Main caption annotations (used in training)
    ‚îú‚îÄ‚îÄ ExpertAnnotations.txt       # Expert-written captions
    ‚îú‚îÄ‚îÄ CrowdFlowerAnnotations.txt  # Crowd-sourced caption quality ratings
    ‚îú‚îÄ‚îÄ Flickr_8k.trainImages.txt  # Training set image list
    ‚îú‚îÄ‚îÄ Flickr_8k.devImages.txt    # Development/validation set image list
    ‚îî‚îÄ‚îÄ Flickr_8k.testImages.txt   # Test set image list
```
---

## Usage

### 1. Open the Notebook

Open the training notebook in your preferred environment (e.g., Jupyter, Colab, VS Code).

### 2. Execute the Complete Training Pipeline

- Run all cells sequentially to go through all **4 phases** (see [Training Pipeline Overview](#training-pipeline-overview) )
- **Dataset (~1GB)** downloads automatically ‚Äì no manual setup required
- **Vocabulary building** and **preprocessing** are handled internally

### 3. Monitor Training Progress

- Real-time **loss/accuracy metrics** displayed during training
- **BLEU-4** and **CIDEr** scores computed during validation
- **Best model** is automatically saved as: `models/best_caption_model.keras`
- **Early stopping** ensures optimal performance and prevents overfitting
- Estimated training time: **~1 hour on GPU** & **~2‚Äì3 hours on CPU**

### 4. Test Your Trained Model

- Automatic validation testing on **random images** after training
- **Interactive testing functions** available for custom images:
```
test_image("path/to/image.jpg")  # Standard caption generation
test_with_sampling("path/to/image.jpg", temperature=0.8)  # Creative captions
```
- **Final model** is saved as:`models/final_caption_model.keras`
- Comprehensive evaluation metrics displayed: `BLEU-4: ~0.19` , `CIDEr: ~0.14`

---

## Results

| Image | Generated Caption |
|-------|------------------|
| ![Dog in grass](https://github.com/user-attachments/assets/07743043-bf9a-4738-af91-0725177f384b) | **Generated Caption:** *a black and white dog is running through the grass* |
| ![Dog in water](https://github.com/user-attachments/assets/1c04991d-205e-42bb-befd-e839ab9bd3b4) | **Generated Caption:** * a black dog is jumping into the water* |
| ![Girl on a bike](https://github.com/user-attachments/assets/539a8058-4eb1-48b1-a892-852a97bc2652) | **Generated Caption:** *a young boy rides his bike down a hill* |


### Metric Interpretation:
- **BLEU-4 (0.1926)**: Competitive score for Flickr8K (typical range: 0.15-0.35)
- **CIDEr (0.1395)**: Room for improvement (typical range: 0.3-1.2)
- **Training Accuracy**: ~85-90% on final epochs
- **Validation Accuracy**: ~80-85% with good generalization

### Generation Modes

#### `test_image()` ‚Äì Standard Caption Generation
- Always generates the same caption for the same image  
- Uses the most confident/probable word at each step  
- Produces reliable, consistent results  
- Best for when you need predictable captions  

#### `test_with_sampling()` ‚Äì Creative Caption Generation
- Generates different captions each time you run it  
- Uses temperature parameter to control randomness  
- Lower temperature (e.g., `0.3`) = more conservative, similar to standard  
- Higher temperature (e.g., `0.8‚Äì1.0`) = more creative and varied captions  
- Best for when you want diverse caption options  

---

## References

**[1]** Vaswani, Ashish, et al. ["Attention is all you need."](https://arxiv.org/abs/1706.03762) Advances in neural information processing systems 30 (2017).

**[2]** Tan, Mingxing, and Quoc V. Le. ["EfficientNet: Rethinking model scaling for convolutional neural networks."](https://arxiv.org/abs/1905.11946) International conference on machine learning. PMLR, 2019.

**[3]** Vinyals, Oriol, et al. ["Show and tell: A neural image caption generator."](https://arxiv.org/abs/1411.4555) Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.

**[4]** Xu, Kelvin, et al. ["Show, attend and tell: Neural image caption generation with visual attention."](https://arxiv.org/abs/1502.03044) International conference on machine learning. PMLR, 2015.

---

## License

MIT License. See [LICENSE](LICENSE) file for details.

---

## Contributing

Contributions are welcome! Here are some areas where you can help improve the project:

- **Beam Search Decoding**  
  Replace greedy decoding with beam search for more diverse and accurate caption generation.

- **Attention Visualization**  
  Add attention map visualization to highlight which parts of the image the model is focusing on during caption generation.

- **Pre-trained Model Support**  
  Create an easy-to-use inference pipeline with downloadable pre-trained weights for quick deployment.

- **Additional Evaluation Metrics**  
  Implement support for advanced metrics such as **METEOR**, **ROUGE-L**, and **SPICE** to provide a more comprehensive evaluation.

- **Vision Transformer Integration**  
  Experiment with using a **ViT (Vision Transformer)** backbone instead of the traditional CNN to compare performance.

- **Cross-Dataset Training**  
  Extend support to other datasets like **MS COCO** and **Flickr30K** to improve generalization and robustness.

### How to Contribute

1. **Fork** the repository to your GitHub account.  
2. **Create a new feature branch** for your enhancement or fix. (`git checkout -b feature/beam-search`)
3. **Implement** your changes or new feature.  
4. **Add tests and documentation** to support your code.  
5. **Submit a pull request** with a clear and detailed description of your changes.

---

**‚≠ê If this implementation helps your research or project, please consider giving it a star!**






